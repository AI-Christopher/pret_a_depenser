{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c65dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.modeling import custom_business_cost_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Les mod√®les choisis pour l'optimisation\n",
    "from lightgbm import LGBMClassifier # Supposons que LightGBM soit notre meilleur candidat\n",
    "# from xgboost import XGBClassifier # Ou XGBoost\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "from functools import partial\n",
    "import warnings\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Masquer les warnings MLflow li√©s √† l'environnement\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Failed to resolve installed pip version.*\")\n",
    "logging.getLogger(\"mlflow.utils.environment\").setLevel(logging.ERROR)\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b69a492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donn√©es charg√©es. Forme: (307507, 139)\n",
      "X shape: (307507, 138), y shape: (307507,)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '../datas/02_preprocess/datas.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Donn√©es charg√©es. Forme: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erreur: Le fichier {DATA_PATH} n'a pas √©t√© trouv√©. V√©rifier le chemin.\")\n",
    "\n",
    "# S√©paration des features (X) et de la cible (y)\n",
    "if 'TARGET' in df.columns:\n",
    "    X = df.drop('TARGET', axis=1)\n",
    "    y = df['TARGET']\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "else:\n",
    "    print(\"Erreur: La colonne 'TARGET' n'a pas √©t√© trouv√©e. V√©rifier le nom de la colonne cible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6764c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de l'ensemble d'entra√Ænement : 246005 √©chantillons\n",
      "Taille de l'ensemble de test : 61502 √©chantillons\n",
      "Proportion de d√©parts dans y_train : 8.07%\n",
      "Proportion de d√©parts dans y_test : 8.07%\n"
     ]
    }
   ],
   "source": [
    "# Division initiale pour avoir un ensemble de test final non touch√© par la CV\n",
    "# Nous utiliserons X_train_full et y_train_full pour la validation crois√©e\n",
    "X_train_full, X_test_final, y_train_full, y_test_final = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Taille de l'ensemble d'entra√Ænement : {X_train_full.shape[0]} √©chantillons\")\n",
    "print(f\"Taille de l'ensemble de test : {X_test_final.shape[0]} √©chantillons\")\n",
    "print(f\"Proportion de d√©parts dans y_train : {y_train_full.mean():.2%}\")\n",
    "print(f\"Proportion de d√©parts dans y_test : {y_test_final.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6561f1e",
   "metadata": {},
   "source": [
    "# Optimisation des hyperparam√®tres et du seuil de d√©cision\n",
    "\n",
    "Objectifs:\n",
    "- Optimiser les hyperparam√®tres via GridSearchCV en maximisant un score align√© m√©tier (co√ªt n√©gatif).\n",
    "- D√©terminer le seuil de d√©cision minimisant le co√ªt m√©tier sur le test final.\n",
    "\n",
    "Fonction de co√ªt (FN > FP):\n",
    "$\\\\text{total\\\\_cost} = FP \\\\cdot COST\\\\_{FP} + FN \\\\cdot COST\\\\_{FN}$\n",
    "\n",
    "√âtapes:\n",
    "1. Split train/test final (test_final tenu hors CV).\n",
    "2. GridSearchCV avec scorer \"co√ªt m√©tier\" et validation crois√©e stratifi√©e.\n",
    "3. Logging MLflow:\n",
    "   - meilleurs hyperparam√®tres, meilleur co√ªt (CV)\n",
    "   - mod√®le (pipeline) + signature + input_example\n",
    "4. Balayage de seuils sur test_final pour minimiser le co√ªt.\n",
    "5. Rapport des m√©triques au seuil optimal + log du plot co√ªt vs seuil.\n",
    "\n",
    "Remarques:\n",
    "- LightGBM: is_unbalance=True pour g√©rer le d√©s√©quilibre.\n",
    "- Pour d'autres mod√®les: class_weight ou SMOTE.\n",
    "- Conserver les figures (plots) en artefacts pour l'auditabilit√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27053428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 15:39:18 INFO mlflow.tracking.fluent: Experiment with name 'Credit_Scoring_Hyperparameter_Optimization' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Experiment set to: Credit_Scoring_Hyperparameter_Optimization\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"Credit_Scoring_Hyperparameter_Optimization\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"MLflow Experiment set to: {experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b80c72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co√ªt Faux N√©gatif (FN): 10000\n",
      "Co√ªt Faux Positif (FP): 1000\n"
     ]
    }
   ],
   "source": [
    "# D√©finis les co√ªts m√©tier. Ces valeurs doivent √™tre ajust√©es avec l'√©quipe m√©tier.\n",
    "COST_FN = 10000  # Co√ªt √©lev√© de la perte d'argent due √† un d√©faut non d√©tect√©\n",
    "COST_FP = 1000   # Co√ªt de l'opportunit√© manqu√©e ou de l'insatisfaction client\n",
    "\n",
    "def business_cost_score(estimator, X, y_true, threshold=0.5, COST_FN=COST_FN, COST_FP=COST_FP):\n",
    "    proba = estimator.predict_proba(X)\n",
    "    if hasattr(proba, \"ndim\") and proba.ndim == 2:\n",
    "        proba = proba[:, 1]\n",
    "    y_pred = (proba >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    total_cost = (fp * COST_FP) + (fn * COST_FN)\n",
    "    return -total_cost  # sklearn maximise le score\n",
    "\n",
    "# Scorer: on minimise le co√ªt => greater_is_better=False\n",
    "business_scorer = partial(business_cost_score, threshold=0.5, COST_FN=COST_FN, COST_FP=COST_FP)\n",
    "\n",
    "print(f\"Co√ªt Faux N√©gatif (FN): {COST_FN}\")\n",
    "print(f\"Co√ªt Faux Positif (FP): {COST_FP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e11de94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Lancement de GridSearchCV pour LightGBM ---\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "Meilleurs hyperparam√®tres trouv√©s: {'model__is_unbalance': True, 'model__learning_rate': 0.05, 'model__max_depth': 7, 'model__n_estimators': 300, 'model__num_leaves': 31}\n",
      "Meilleur co√ªt m√©tier (moyenne CV): 25706800.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'LightGBM_CreditScoring_Optimized'.\n",
      "2025/10/02 15:42:50 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LightGBM_CreditScoring_Optimized, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod√®le optimis√© logg√© dans MLflow Registry sous le nom: LightGBM_CreditScoring_Optimized\n",
      "üèÉ View run LightGBM_HPO_GridSearch at: http://localhost:5000/#/experiments/791806172091042918/runs/bb8671abbdbf480182f233f79afb7dd3\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/791806172091042918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'LightGBM_CreditScoring_Optimized'.\n"
     ]
    }
   ],
   "source": [
    "# Mod√®le candidat pour l'optimisation\n",
    "model_class = LGBMClassifier\n",
    "model_name = \"LightGBM\"\n",
    "\n",
    "# D√©finition du pipeline\n",
    "# LightGBM n'est pas sensible √† la mise √† l'√©chelle, donc le scaler est optionnel ici.\n",
    "# Si tu avais un mod√®le comme LogisticRegression ou MLP, tu le mettrais.\n",
    "pipeline = Pipeline([\n",
    "    # ('scaler', StandardScaler()), # D√©commenter si le scaler est b√©n√©fique pour ton mod√®le choisi\n",
    "    ('model', model_class(random_state=42, n_jobs=-1, verbosity=-1, force_row_wise=True))\n",
    "])\n",
    "\n",
    "# Param√®tres √† optimiser pour LightGBM\n",
    "# Adapte ce dictionnaire de param√®tres en fonction de tes besoins et de ton mod√®le.\n",
    "# C'est un exemple, il faut √™tre un peu plus agressif pour une vraie optimisation.\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__learning_rate': [0.05, 0.1],\n",
    "    'model__num_leaves': [20, 31, 40],\n",
    "    'model__max_depth': [-1, 7], # -1 signifie pas de limite\n",
    "    'model__is_unbalance': [True], # Gestion du d√©s√©quilibre\n",
    "    # 'model__scale_pos_weight': [ (y_train_full == 0).sum() / (y_train_full == 1).sum() ] # Alternative pour XGBoost\n",
    "}\n",
    "\n",
    "# Initialisation de StratifiedKFold pour la validation crois√©e dans GridSearchCV\n",
    "cv_stratified = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"\\n--- Lancement de GridSearchCV pour {model_name} ---\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{model_name}_HPO_GridSearch\"):\n",
    "    # Log des param√®tres du GridSearch\n",
    "    mlflow.log_param(\"model_optimized\", model_name)\n",
    "    mlflow.log_param(\"scoring_metric_hpo\", \"negative_business_cost\")\n",
    "    mlflow.log_param(\"cv_n_splits\", cv_stratified.n_splits)\n",
    "    mlflow.log_dict(param_grid, \"param_grid\")\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring=business_scorer,  # <-- utiliser le callable\n",
    "        cv=cv_stratified,\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "        error_score=\"raise\"\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_full, y_train_full)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = -grid_search.best_score_  # reconvertir en co√ªt positif (car sklearn a invers√© le signe)\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    print(f\"\\nMeilleurs hyperparam√®tres trouv√©s: {best_params}\")\n",
    "    print(f\"Meilleur co√ªt m√©tier (moyenne CV): {best_score:.2f}\")\n",
    "\n",
    "    # Log des meilleurs param√®tres et score dans MLflow\n",
    "    mlflow.log_params({f\"best_{k}\": v for k, v in best_params.items()})\n",
    "    mlflow.log_metric(\"best_cv_business_cost\", best_score)\n",
    "    mlflow.set_tag(\"grid_search_status\", \"completed\")\n",
    "\n",
    "    # Enregistrer le mod√®le optimis√© (le pipeline complet)\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=best_estimator,\n",
    "        name=f\"{model_name}_optimized_pipeline\",\n",
    "        registered_model_name=f\"{model_name}_CreditScoring_Optimized\",\n",
    "        input_example=X_train_full.iloc[:5],\n",
    "        signature=infer_signature(X_train_full, best_estimator.predict_proba(X_train_full)[:, 1])\n",
    "    )\n",
    "\n",
    "    print(f\"Mod√®le optimis√© logg√© dans MLflow Registry sous le nom: {model_name}_CreditScoring_Optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f79b7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optimisation du seuil de d√©cision sur l'ensemble de test final ---\n",
      "Seuil optimal trouv√© : 0.50\n",
      "Co√ªt m√©tier minimum √† ce seuil sur le test final : 31767000.00\n",
      "\n",
      "--- M√©triques sur le test final avec le seuil optimal (0.50) ---\n",
      "  ROC AUC: 0.7671\n",
      "  F1-Score: 0.2785\n",
      "  Recall (classe 1): 0.6814\n",
      "  Precision (classe 1): 0.1750\n",
      "  Vrais Positifs (TP): 3383\n",
      "  Faux Positifs (FP): 15947\n",
      "  Faux N√©gatifs (FN): 1582\n",
      "  Vrais N√©gatifs (TN): 40590\n",
      "Plot du co√ªt vs seuil sauvegard√© et logg√©.\n",
      "üèÉ View run LightGBM_Threshold_Optimization_Final_Metrics at: http://localhost:5000/#/experiments/791806172091042918/runs/865d9271dd3f48faa775f5394500beb0\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/791806172091042918\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Optimisation du seuil de d√©cision sur l'ensemble de test final ---\")\n",
    "\n",
    "# Utilise le meilleur estimateur trouv√© par GridSearchCV\n",
    "final_model_pipeline = best_estimator\n",
    "\n",
    "# Obtenir les probabilit√©s sur l'ensemble de test final\n",
    "y_pred_proba_test = final_model_pipeline.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "# G√©n√©rer une gamme de seuils √† tester\n",
    "thresholds = np.arange(0.01, 0.99, 0.01) # De 0.01 √† 0.98 par pas de 0.01\n",
    "\n",
    "cost_at_threshold = []\n",
    "for t in thresholds:\n",
    "    cost = -custom_business_cost_scorer(y_test_final, y_pred_proba_test, threshold=t)\n",
    "    cost_at_threshold.append(cost)\n",
    "\n",
    "# Trouver le seuil qui minimise le co√ªt\n",
    "optimal_threshold_idx = np.argmin(cost_at_threshold)\n",
    "optimal_threshold = thresholds[optimal_threshold_idx]\n",
    "min_business_cost = cost_at_threshold[optimal_threshold_idx]\n",
    "\n",
    "print(f\"Seuil optimal trouv√© : {optimal_threshold:.2f}\")\n",
    "print(f\"Co√ªt m√©tier minimum √† ce seuil sur le test final : {min_business_cost:.2f}\")\n",
    "\n",
    "# Re-calculer les m√©triques classiques avec le seuil optimal\n",
    "y_pred_optimal = (y_pred_proba_test >= optimal_threshold).astype(int)\n",
    "optimal_roc_auc = roc_auc_score(y_test_final, y_pred_proba_test)\n",
    "optimal_f1 = f1_score(y_test_final, y_pred_optimal)\n",
    "optimal_recall = recall_score(y_test_final, y_pred_optimal)\n",
    "optimal_precision = precision_score(y_test_final, y_pred_optimal)\n",
    "optimal_tn, optimal_fp, optimal_fn, optimal_tp = confusion_matrix(y_test_final, y_pred_optimal).ravel()\n",
    "\n",
    "print(f\"\\n--- M√©triques sur le test final avec le seuil optimal ({optimal_threshold:.2f}) ---\")\n",
    "print(f\"  ROC AUC: {optimal_roc_auc:.4f}\")\n",
    "print(f\"  F1-Score: {optimal_f1:.4f}\")\n",
    "print(f\"  Recall (classe 1): {optimal_recall:.4f}\")\n",
    "print(f\"  Precision (classe 1): {optimal_precision:.4f}\")\n",
    "print(f\"  Vrais Positifs (TP): {optimal_tp}\")\n",
    "print(f\"  Faux Positifs (FP): {optimal_fp}\")\n",
    "print(f\"  Faux N√©gatifs (FN): {optimal_fn}\")\n",
    "print(f\"  Vrais N√©gatifs (TN): {optimal_tn}\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{model_name}_Threshold_Optimization_Final_Metrics\", nested=True):\n",
    "    mlflow.log_param(\"final_optimal_threshold\", optimal_threshold)\n",
    "    mlflow.log_metric(\"final_min_business_cost\", min_business_cost)\n",
    "    mlflow.log_metric(\"final_roc_auc\", optimal_roc_auc)\n",
    "    mlflow.log_metric(\"final_f1_score\", optimal_f1)\n",
    "    mlflow.log_metric(\"final_recall\", optimal_recall)\n",
    "    mlflow.log_metric(\"final_precision\", optimal_precision)\n",
    "    mlflow.log_metric(\"final_tp\", optimal_tp)\n",
    "    mlflow.log_metric(\"final_fp\", optimal_fp)\n",
    "    mlflow.log_metric(\"final_fn\", optimal_fn)\n",
    "    mlflow.log_metric(\"final_tn\", optimal_tn)\n",
    "    mlflow.set_tag(\"stage\", \"final_evaluation_with_optimal_threshold\")\n",
    "\n",
    "    # Plot du co√ªt en fonction du seuil\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, cost_at_threshold, marker='o', linestyle='-', markersize=4)\n",
    "    plt.axvline(x=optimal_threshold, color='r', linestyle='--', label=f'Seuil Optimal = {optimal_threshold:.2f}')\n",
    "    plt.title('Co√ªt M√©tier en fonction du Seuil de Classification')\n",
    "    plt.xlabel('Seuil de Probabilit√©')\n",
    "    plt.ylabel('Co√ªt M√©tier Total')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Sauvegarder la figure comme artefact MLflow\n",
    "    plot_path = \"cost_vs_threshold_plot.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    mlflow.log_artifact(plot_path)\n",
    "    plt.close() # Fermer la figure pour ne pas encombrer la m√©moire si plusieurs plots sont g√©n√©r√©s\n",
    "    print(f\"Plot du co√ªt vs seuil sauvegard√© et logg√©.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pret_a_depenser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
